% !Mode:: "TeX:UTF-8"

\chapter{Introduction}

\section{Background}
The theory and application of electromagnetic wave has been developed greatly in last about one hundred years since the Maxwell's equations were established. The researches and applications in the area of electromagnetic wave have dived into every sub-area,  like electromagnetic scattering, electromagnetic radiation, modeling of waveguides, electromagnetic imaging, and electromagnetic probe. In real environment, the process of propagation of electromagnetic wave is very complicated, like the scattering of an complex object, the real communication in city at a complicated topography, and the propagation in waveguide. So, it is very useful if we can know the specific feature of electromagnetic wave under some real environments like we mentioned before and we already have two important ways to do it, which are simulating and theoretical analyzing. The theoretical analyzing method, however, can only answer some typical problems, not too complex problems derived from real environments and involving real electromagnetic arguments. Unfortunately, we always have to deal with the later one, which means that we have to use the simulating way to solve those problems derived from real circumstances, and motivated the development of computational electromagnectics. So far, there are many computational numerical techniques to overcome the inability of analytically calculation to derive closed solutions of Maxwell's equation, like Methos of Moments (MoM), Boundary Element Method (BEM), Fast Multipole Method (FMM), Finite Element Method (FEM), and Finite Difference Time Domain (FDTD).

In 1996, K. S. Yee \cite{Yee} present the FDTD scheme in his seminal paper. This method discretiz time-dependent Maxwell's equations by applying centered finite difference operators on staggered grids in space and time for each electric and magnetic vector field component in Maxwell's equations. Then the resulting finite difference equations are solved in a leapfrog manner: calculating the electric field vector components in a volume of space at a given instant; then do the same thing to the magnetic field vector component in the same spatial area at the next instant in time; repeating the process over and over again until the desired transient.

As the power of computer is finite, we can only to computation to simulate the electromagnetic wave in a finite volume space. So, if we want to simulate the propagation of electromagnetic wave for open-region FDTD, we have to give out an appropriate boundary condition. To solve this problem, there are many solutions. In 1969, Taylor\cite{taylor} et al. presented a extrapolation boundary condition. In 1981, Mur\cite{mur1981absorbing} proposed a absorbing boundary condition (ABC). Berenger \cite{mur1981absorbing} developed the perfectly match layer (PML) in a seminal 1994 paper. Based on the original work of Berenger, various PML formulation were proposed. One of the most famous modified and extended implementation is uniaxial PML (UPML), which was proposed by Sacks et al. \cite{sacks} and Gedney et al. \cite{gedney}. Another famous modified PML is convolutional PML (CPML), which was proposed by Chew and Weedon et al. \cite{gedney}. The Mur, UPML, and CPML are most commonly used ABCs. The Mur ABC is the most simple one among them, and the latter two PML ABCs have more strong ability to absorb evanescent waves.

\section{Value}
 FDTD has many strengths, so it became the primary means to solve those problems and got a wide range of applications like radar signature technolofy, antennas, wireless communications devices, digital interconnections, even to photonic crystals, solitons, and biophotonics.

To implement the FDTD, we need to establish grids in the computational domain at first. Concerned the Courant condition, the grid spatial discretization must be fine enough to resolve the smallest wavelength, which means the number of grids in the computational domain has a low boundary. Hence, we have a very big computation task as the significant number of grids as we need to compute every grid at a given instant. So, objects with long and thin feature are not suitable to model by using FDTD. As the size of grid, the time step need to fit into the Courant condition, too. So, we have to wait a long time if the desired transient is far form the initial time. That is the second problem of FDTD: it always take too much time to solve a problem.

Parallel processing is the main answer to solve the second problem. FDTD have a natural character of parallelism, that is, the FDTD method requires only exchanging field on the interface between adjacent sub-domains. In this paper , we introduced two main method of accelerating FDTD by hardwards, which make FDTD more useful in practical enviroment.


\section{Present Condition}

According to this parallelism nature of FDTD, some parallel FDTD algorithm\cite{mpi1,mpi2,mpi3,mpi4} has been developed. In those parallel method, the original computational domain has been divided into some sub-domains and assign each sub-domain to a individual computational core. The computational core complete its own work independently at any give instant, and then exchange the data of sub-domain's boundary with its adjacent computational core before the next given instant. One famous example of this method is message passing interface (MPI).

The MPI method, and other method who adopt the way dividing original computational domain belong to task-level parallelism (TLP), which involves the decomposition of a task into sub-tasks and then complete these sub-tasks simultaneously and sometimes cooperatively. The instruction-level parallelism (ILP), which is lower than TLP, can be achieved by all modern CPU automatically without human's intervention. The lowest level parallelism, data-level parallelism (DLP), still has some potential power of computation of FDTD. In this paper, we present a scheme to make the potential power of computation used more fully. Besides, our view beyond the CPU system itself, trying to find other independent which can accelerate the massive computation tasks. So, we also show how to use CUDA to make FDTD faster and how significant power of computation the CUDA have.

People used to utillize CPU directly to compute. In this way, the computations are completed by arithmetic logic unit (ALU), which can only compute one data at once. To achieve data parallelism, we use vector processor (VP) instead of ALU to compute. A vector processor is a  central processing unit that can operate on one-dimensional arrays of data which called vectors by using instructions. Vector processor improves much about the performance of computation in some areas, especially in numerical simulation tasks. Now, the instruction set to operate vector is provided by
almost commercial CPUs, like Intel or AMD, provided VIS, MX, SSE, and AVX, etc. Therefore one of the advantages of VP is easy to access.

In past researches about accelerating the FDTD method based on data parallelism. L. Zhang and W. Yu \cite{LZhangandWYu} used SSE to accelerate 3D FDTD method with single precision floating point arithmetic. M. Livesey, F. Costen, and X. Yang \cite{Doubleprecision} extended the work to doubleprecision floating arithmetic. The way to use vector processor and the pseudocode were
presented by Y. L et al. \cite{AdvancedFDTDMethod}.

Though in every parallelism level we have schemes to accelerate the FDTD, the CPU is designed to deal with every possible complex general operation by higher operation frequency, more registers, and more advanced ALU, not to solve big data computation task professionally. So, it cannot make full use of CPU and meet the need of efficiency of FDTD if we use CPU to execute FDTD. If we concentrate our attention on graphic processor unit (GPU), we will find that GPU, which has hundreds of computational core, and was designed to deal with big volume data and repetitive computation, is a better choice to execute FDTD.

The concept of GPU was proposed in 1999. Sean E. Krakiwsky et al.\cite{Krakiwsky} tried to use GPU to accelerate FDTD at the first time in 2004. At that time, the GPUs were not designed to general purpose computation, so it was difficult to use GPU to complete FDTD as people who want to use it need to understand the architecture of it and a special hardware language. Things got better in 2006, as GPU had already had general purpose computational architecture at that time. In 2007, the computational unified device architecture (CUDA) was published, which is a milestone. The presence of CUDA allowed people use the programming languages they have already learned, like Fortran, C, or C++, which speed up the research about using GPU on FDTD. J. A. Roden and S. Gedney\cite{Roden} proposed a implementation of FDTD under the CPML ABC. However, they did not provide any details. Veysel Demir \cite{Demir} proposed a implementation of FDTD under periodic boundary condition (PBC). Tomasz Dziubak et al.\cite{dgffdtd} proposed a implementation of DGF-FDTD. M. R. Zunoubi, J. Payne, and W. P. Roach\cite{tez} presents the GPU implementation of the FDTD method for the solution of the two-dimensional electromagnetic fields inside dispersive media. M. Livesey et al.\cite{decomposition} introduced an alternative domain decomposition technique for CUDA-based FDTD methods

\section{Topics}

Though there have been some researches about using a set of instructions to speed up FDTD, all of them ignored the differences of the characters between different boundary conditions remain some potencies to be exploit. We found that the computational domain could be modified to enhance the efficiency of computation for some specific boundary conditions, like MUR. Considering the characters of some ABCs, like Mur ABC, we give out a new computational model and its implementation developed by C++ under 2D FDTD condition as an example and analyze the performance by using Visual Studio. This model could be extended to 3D FDTD method.

 In this paper, we also use CUDA to accelerate 2D FDTD under Mur ABC to evaluate the performance to CUDA. As a device which is  independent, designed to handle massive, repeated computations, GPU is very different from CPU, and calling GPU has not been considered by CPU manufacturers. So, we described the details of optimizing the processes of using CUDA, and designed the FDTD algorithm prudently on CUDA.
